{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  #for visualization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instructions for the students using Google Colab\n",
    "### If you are using Google Colab, you will need to first mount the google drive. You can do this by running the following code after uncommenting it.\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "### You also need to upload the data to your Google drive.\n",
    "### Make sure you are using the same google account for both Google Colab and Google Drive.\n",
    "\n",
    "### For example, if the dataset data.npy is located in /content/drive/My Drive/CME216/HW5/PartA, then you can use the following code to load the data.\n",
    "# data_path = '/content/drive/MyDrive/CME216/HW5/PartA'\n",
    "# data = np.load(f'{data_path}/data.npy')\n",
    "### OR else.....\n",
    "### you can cd to the directory where the data is located using the following command\n",
    "# %cd /content/drive/MyDrive/CME216/HW5/PartA\n",
    "### And then load the data. Make sure that the notebook is running in the same directory where the data is located.\n",
    "# data = np.load('data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X = np.load(\"u0_data.npy\")\n",
    "Y = np.load(\"uT_data.npy\")\n",
    "\n",
    "# split data into training, validation and test sets\n",
    "X_flatten = X.reshape(len(X), -1)\n",
    "Y_flatten = Y.reshape(len(Y), -1)\n",
    "\n",
    "train_split, val_split = int(len(X)*0.8), int(len(X)*0.9)\n",
    "X_train, X_test, X_validation = X_flatten[:train_split], X_flatten[train_split:val_split], X_flatten[val_split:]\n",
    "Y_train, Y_test, Y_validation = Y_flatten[:train_split], Y_flatten[train_split:val_split], Y_flatten[val_split:]\n",
    "\n",
    "# create corresponding PyTorch tensors\n",
    "print(X_flatten.dtype)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(X_validation), torch.from_numpy(Y_validation))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test))\n",
    "\n",
    "# create Pytorch dataloaders\n",
    "train_loader = DataLoader(train_dataset)\n",
    "validation_loader = DataLoader(validation_dataset)\n",
    "test_loader = DataLoader(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple fully connected network class with following arguments:\n",
    "# input_dim: input dimension\n",
    "# output_dim: output dimension\n",
    "# n_layers: number of layers (counted as number of hidden layers + 1 output layer)\n",
    "# n_units: number of neurons in each layer\n",
    "# activation: type of activation function (NOTE: there should be no activation function in the output layer)\n",
    "\n",
    "class FullyConnectedNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_layers, n_units, activation):\n",
    "        super(FullyConnectedNetwork, self).__init__()  #we create a temporaty object of superclass(parent class), and then call the constructor to initialize \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(input_dim,n_units))\n",
    "        for _ in range( n_layers-1):\n",
    "            self.layers.append(torch.nn.Linear(n_units,n_units))\n",
    "        self.layers.append(torch.nn.Linear(n_units,output_dim))\n",
    "        self.act = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:  #no activation in last layers\n",
    "            x = self.act(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "You may use the following functions in train_and_plot function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute relative L2 error in percentage\n",
    "def rel_l2_error(pred, true):\n",
    "    \"\"\"A helper function to compute the relative L2 error in percentage\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted values\n",
    "        true (torch.Tensor): True values\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Relative L2 error in percentage\n",
    "    \"\"\"\n",
    "    return (torch.norm(pred - true) / torch.norm(true))*100\n",
    "\n",
    "\n",
    "# prediction plotting function\n",
    "def prediction_plots(n_plots, indices, u0, uT, output):\n",
    "    \"\"\"A helper function to plot the predictions of the model\n",
    "\n",
    "    Args:\n",
    "        n_plots (int): Number of plots to display\n",
    "        indices (list): List of indices to plot\n",
    "        u0 (np.array): Initial condition (3D numpy array of shape (n_samples, D, D))\n",
    "        uT (np.array): Target condition (3D numpy array of shape (n_samples, D, D))\n",
    "        output (np.array): Model output (3D numpy array of shape (n_samples, D, D))\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(n_plots, 4, figsize=(20, 5*n_plots))\n",
    "    for i, idx in enumerate(indices):\n",
    "        im = axs[i, 0].imshow(u0[idx, :, :], cmap='viridis')\n",
    "        divider = make_axes_locatable(axs[i, 0])\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        im = axs[i, 1].imshow(uT[idx, :, :], cmap='viridis')\n",
    "        divider = make_axes_locatable(axs[i, 1])\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        im = axs[i, 2].imshow(output[idx, :, :], cmap='viridis')\n",
    "        divider = make_axes_locatable(axs[i, 2])\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        im = axs[i, 3].imshow(output[idx, :, :]-uT[idx, :, :], cmap='viridis')\n",
    "        divider = make_axes_locatable(axs[i, 3])\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    axs[0, 0].set_title('Initial condition (u(x, 0))')\n",
    "    axs[0, 1].set_title('Target condition (u(x, T))')\n",
    "    axs[0, 2].set_title('DNN prediction (u_pred(x, T))')\n",
    "    axs[0, 3].set_title('Error')\n",
    "    for ax in axs.flatten():\n",
    "        ax.axis('off')\n",
    "    plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write train_and_plot function here...\n",
    "\n",
    "# 10 points. Training routine: Write a function called train and plot which takes as argument\n",
    "# following things:\n",
    "# • model (model – an object instantiation of the class FullyConnectedNetwork)\n",
    "# • optimizer (type of optimizer for performing gradient updates)\n",
    "# • max epochs (maximum number of epochs)\n",
    "# • batch size (number of samples in each batch)\n",
    "def train_and_plot(model, optimizer, max_epoches, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    training_loss_list = []\n",
    "    validation_loss_list =[]\n",
    "    total_train_loss = 0.0\n",
    "    model.to(device)\n",
    "    for epoch in range(max_epoches):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for X_batch, Y_batch in  train_loader:\n",
    "            X_batch, Y_batch = X_batch.to(device).float(), Y_batch.to(device).float()   # change dtype to float32 to match the model\n",
    "            # print(X_batch.device)\n",
    "            # print(next(model.parameters()).device)\n",
    "            print(X_batch.dtype)\n",
    "            # print(Y_batch.dtype)\n",
    "            outputs = model(X_batch)\n",
    "            training_loss = criterion(outputs,Y_batch)\n",
    "            total_train_loss += training_loss.item()  #converting tensor with single value to float, will raise an error if there's multiple items\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        average_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        if (epoch + 1)%1 == 0:\n",
    "            model.eval()   #ensure that all layers are in inference mode.\n",
    "            with torch.no_grad():  #temporarily set all reuire_gradient to false\n",
    "                total_validation_loss = 0.0\n",
    "                total_RE = 0.0\n",
    "\n",
    "                for X_batch, Y_batch in validation_loader:\n",
    "                    X_batch, Y_batch = X_batch.to(device).float(), Y_batch.to(device).float()\n",
    "                    outputs = model(X_batch)\n",
    "                    validation_loss = criterion(outputs, Y_batch)\n",
    "                    total_validation_loss += validation_loss.item()\n",
    "                    total_RE +=  rel_l2_error(outputs, Y_batch)\n",
    "                average_valid_loss = total_validation_loss / len(validation_loader)\n",
    "                \n",
    "\n",
    "                training_loss_list.append(average_train_loss)\n",
    "                validation_loss_list.append(average_valid_loss)\n",
    "\n",
    "                MRE = total_RE/ len(validation_loader)\n",
    "                \n",
    "\n",
    "                print(f\"Current Epoch: {epoch}/{max_epoches}\") # f stands for \"formatted string literal\" \n",
    "                print(f\"The training loss is {average_train_loss}\") #loss itself is a tensor\n",
    "                print(f\"The validation loss is {average_valid_loss}\") #loss itself is a tensor\n",
    "                print(f\"The mean relative error (in percentage) for the validation set: {MRE}\")\n",
    "\n",
    "    print('hi')\n",
    "    print(training_loss_list)\n",
    "    print(validation_loss_list)\n",
    "    plt.figure()\n",
    "    plt.semilogy(training_loss_list, label = \"Training Loss\")\n",
    "    plt.semilogy(validation_loss_list, label = \"Validation Loss\")\n",
    "    # plt.title(\"Loss history of training and validation test\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss value\")\n",
    "\n",
    "    # epoches = np.arange(0, max_epoches+1, 100)\n",
    "    # plt.xticks(epoches)\n",
    "\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # n_plots = 10\n",
    "    # random_indexs = np.random.choice(len(test_loader), n_plots, replace=False)\n",
    "    # test_inputs, test_outputs, predict_outputs = [], [], []   # Python lists are versatile and can hold elements of different types\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for index in random_indexs:\n",
    "    #         input, output = test_loader.dataset[index]\n",
    "    #         predict = model(input.to(device).float())\n",
    "    #         test_inputs.append(input)\n",
    "    #         test_outputs.append(output)\n",
    "    #         predict_outputs.append(predict)\n",
    "    # prediction_plots(n_plots, random_indexs, test_inputs, test_outputs, predict_outputs)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Current Epoch: 0/2\n",
      "The training loss is 0.6997360810637474\n",
      "The validation loss is 0.7175255417823792\n",
      "The mean relative error (in percentage) for the validation set: 99.92164611816406\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Current Epoch: 1/2\n",
      "The training loss is 0.6996970996260643\n",
      "The validation loss is 0.7174859642982483\n",
      "The mean relative error (in percentage) for the validation set: 99.91886901855469\n",
      "hi\n",
      "[0.6997360810637474, 0.6996970996260643]\n",
      "[0.7175255417823792, 0.7174859642982483]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# write your code here...\n",
    "#For this task, first create a model using the FullyConnectedNetwork class with n layers = 8 and\n",
    "#n units = 700. Set input dim and output dim to be equal to the data dimension (676).\n",
    "\n",
    "model = FullyConnectedNetwork(input_dim=676, output_dim=676, n_layers=8, n_units=700, activation=torch.nn.ReLU())\n",
    "\n",
    "optimizer_SGD = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "train_and_plot(model=model, optimizer=optimizer_SGD, max_epoches=2, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this text cell to: comment on your observations. Which optimizer performs best? Which criteria are you using to\n",
    "judge the relative performance of different optimizers and reach at your conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Tuning learning rate for the optimal optimizer (learning_rate=[1e-5, 1e-2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this text cell to: comment on your observations. Which learning rate performs best? Which criteria are you using to\n",
    "judge the relative performance? Why do you think the specific value of learning rate performed better compared to other two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
